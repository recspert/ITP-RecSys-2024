{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "username = 'recspert'\n",
    "repo = 'ITP-RecSys-2024'\n",
    "\n",
    "# remove local directory if it already exists\n",
    "if os.path.isdir(repo):\n",
    "    !rm -rf {repo}\n",
    "\n",
    "!git clone https://github.com/{username}/{repo}.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, BatchSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from polara import get_movielens_data\n",
    "from polara.lib.sampler import sample_element_wise\n",
    "from polara.tools.random import random_seeds, seed_generator\n",
    "from polara.preprocessing.dataframes import leave_one_out, reindex, sample_unseen_interactions\n",
    "from ipypb import track\n",
    "\n",
    "from dataprep import transform_indices\n",
    "from rndutils import fix_torch_seed\n",
    "from ann_metrics import metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_movielens_data(\"C:/Users/evfro/Downloads/ml-1m.zip\", include_time=True)\n",
    "data_small = data.sample(frac=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_, data_index = transform_indices(data_small, 'userid', 'movieid')\n",
    "training = training_.sort_values(['userid', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description = dict(\n",
    "    users = data_index['users'].name,\n",
    "    items = data_index['items'].name,\n",
    "    n_users = len(data_index['users']),\n",
    "    n_items = len(data_index['items']),\n",
    ")\n",
    "data_description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Matrix Factorization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralMF(nn.Module):\n",
    "    def __init__(self, user_num, item_num, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(item_num, embedding_dim)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        nn.init.normal_(self.user_embeddings.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embeddings.weight, std=0.01)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_embedding = self.user_embeddings(user)\n",
    "        item_embedding = self.item_embeddings(item)\n",
    "\n",
    "        user_embedding = F.normalize(user_embedding)\n",
    "        item_embedding = F.normalize(item_embedding)\n",
    "        matmul = torch.sum(user_embedding*item_embedding, -1)\n",
    "        return matmul.view(-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to define an iterator for our dataset that will sweep trough the observed data and also inject negative samples into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMFDataset(Dataset):\n",
    "    def __init__(self, observations, n_users, n_items, n_samples=None, seed=None):\n",
    "        super().__init__()\n",
    "        self.observations = observations\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.data = None\n",
    "        self.seed = seed\n",
    "        # data sampling initialization\n",
    "        self.observations_mat = self.matrix_from_observations()\n",
    "        self.n_samples = n_samples\n",
    "        self._sampler_state = seed_generator(self.seed)\n",
    "        self._shuffle_state = seed_generator(self.seed)\n",
    "        self.shuffle = True\n",
    "        self.reset_dataset()\n",
    "\n",
    "    def reset_random_state(self):\n",
    "        fix_torch_seed(self.seed)\n",
    "        self._sampler_state.send(self.seed)\n",
    "        self._shuffle_state.send(self.seed)\n",
    "\n",
    "    def reset_dataset(self):\n",
    "        '''This method will be used by pytorch Sampler object'''\n",
    "        if self.n_samples:\n",
    "            negative_examples = self.sample_negatives(next(self._sampler_state))\n",
    "            data = np.concatenate((self.observations, negative_examples), axis=0)\n",
    "            labels = [1] * len(self.observations) + [0] * len(negative_examples)\n",
    "        else:\n",
    "            data = self.observations\n",
    "            labels = [0] * len(self.observations)\n",
    "\n",
    "        self.data = np.concatenate([data, np.array(labels)[:, np.newaxis]], axis=1)\n",
    "\n",
    "        if self.shuffle:\n",
    "            random_state = np.random.RandomState(next(self._shuffle_state))\n",
    "            random_state.shuffle(self.data)\n",
    "\n",
    "    def matrix_from_observations(self):\n",
    "        vals = np.broadcast_to(1, len(self.observations))\n",
    "        rows = self.observations[:, 0]\n",
    "        cols = self.observations[:, 1]\n",
    "        shape = (self.n_users, self.n_items)\n",
    "        return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "    def sample_negatives(self, entropy):\n",
    "        # important note: negative samples must include holdout items as well,\n",
    "        # otherwise the model will be provided with hints about ground truth\n",
    "        samples = sample_element_wise(\n",
    "            # performs \"uniform\" sampling\n",
    "            indptr = self.observations_mat.indptr,\n",
    "            indices = self.observations_mat.indices,\n",
    "            n_cols = self.n_items,\n",
    "            n_samples = self.n_samples,\n",
    "            seed_seq = random_seeds(self.n_users, entropy=entropy)\n",
    "        )\n",
    "        user_index = np.broadcast_to(\n",
    "            np.repeat(\n",
    "                np.arange(self.n_users),\n",
    "                np.diff(self.observations_mat.indptr)\n",
    "            )[:, np.newaxis],\n",
    "            samples.shape\n",
    "        )\n",
    "        return np.concatenate([user_index.flat, samples.flat]).reshape(-1, 2, order='F')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, item, label = self.data[idx]\n",
    "        output = {\n",
    "            \"users\": np.int64(user),\n",
    "            \"items\": np.int64(item),\n",
    "            \"labels\": np.float32(label),\n",
    "        }\n",
    "        return output\n",
    "\n",
    "\n",
    "class SamplerWithReset(RandomSampler):\n",
    "    def __iter__(self):\n",
    "        self.data_source.reset_dataset()\n",
    "        return super().__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_data = training[[\n",
    "    data_description['users'],\n",
    "    data_description['items']\n",
    "]]\n",
    "\n",
    "train_dataset = NeuMFDataset(\n",
    "    observed_data.values,\n",
    "    data_description['n_users'],\n",
    "    data_description['n_items'],\n",
    "    n_samples = 1,\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = 64, # will use the default collation_fn for gathering batches\n",
    "        drop_last = False,\n",
    "        sampler = SamplerWithReset(train_dataset),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: The default `collate_fn` function gathers batches entry-by-entry, it is an IO-bound procedure. Hence, it is more appropriate for ANN's with heavy computations, e.g. a CNN trained on an image dataset. In that case the overhead to gather a batch is small comparing to the compute time.\n",
    "\n",
    "However, in the recsys case with a simple Neural MF model, computations are lightweight and the time spent inside `collate_fn` dominates, see [this issue](https://github.com/pytorch/pytorch/issues/21645) for more details. A viable workaround for not too large datasets is to create a custom dataloader which doesn't rely on `collate_fn` and performs sampling more efficiently in a vectorized form. For example, see [cofida](https://github.com/evfro/cofida) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl:\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, model, optimizer, criterion, scheduler=None, show_progress=True):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    if show_progress:\n",
    "        loader = track(loader)\n",
    "\n",
    "    for batch in loader:\n",
    "        users = batch[\"users\"]\n",
    "        items = batch[\"items\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(users, items), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().cpu().item())\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    dim = 50,\n",
    "    learning_rate = 1e-3,\n",
    "    epochs = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralMF(data_description['n_users'], data_description['n_items'], config['dim'])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if not next(model.parameters()).is_cuda:\n",
    "        model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = config['learning_rate']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in track(range(config['epochs'])):\n",
    "    losses = train(train_dl, model, optimizer, criterion, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(losses).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loader, model, top_k=[10], show_progress=True):\n",
    "    model.eval()\n",
    "    data = defaultdict(list)\n",
    "    coverage_set = set()\n",
    "\n",
    "    if show_progress:\n",
    "        loader = track(loader)\n",
    "\n",
    "    for batch in loader:\n",
    "        users = batch[:, 0]\n",
    "        items = batch[:, 1]\n",
    "        with torch.no_grad():\n",
    "            predictions = model(users, items)\n",
    "            for k in top_k:\n",
    "                hits, mrrs, dcgs = metrics(predictions, top_k=k, coverage_set=coverage_set)\n",
    "                data[f\"hr@{k}\"].append(hits)\n",
    "                data[f\"mrr@{k}\"].append(mrrs)\n",
    "                data[f\"ndcg@{k}\"].append(dcgs)\n",
    "\n",
    "    output = {}\n",
    "    for metric in [\"hr\", \"mrr\", \"ndcg\"]:\n",
    "        for k in top_k:\n",
    "            name = f\"{metric}@{k}\"\n",
    "            output[name] = np.mean(data[name])\n",
    "\n",
    "    for k in top_k:\n",
    "        name = f\"cov@{k}\"\n",
    "        output[name] = len(coverage_set)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout = (\n",
    "    data\n",
    "    .drop(data_small.index)\n",
    "    .pipe(reindex, data_index['items'])\n",
    "    .pipe(leave_one_out, random_state=0)[1]\n",
    "    .pipe(reindex, data_index['users'])\n",
    "    .sort_values(data_description['users'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_negative_samples = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data = sample_unseen_interactions(\n",
    "    pd.concat([training, holdout], axis=0, ignore_index=True),\n",
    "    np.arange(data_description['n_items']),\n",
    "    n_random = test_negative_samples,\n",
    "    random_state = 0,\n",
    "    userid = data_description['users'],\n",
    "    itemid = data_description['items']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data_ = (\n",
    "    holdout\n",
    "    .set_index('userid')\n",
    "    ['movieid']\n",
    "    .combine(unseen_data, lambda x, y: np.r_[x, y])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that holdout items are at the first position for each test user\n",
    "assert (scoring_data_.str[0] == holdout.set_index('userid')['movieid']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data = scoring_data_.explode().reset_index().values.astype('intp')\n",
    "scoring_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in scoring_dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in track(range(config['epochs'])):\n",
    "    losses = train(train_dl, model, optimizer, criterion, show_progress=True)\n",
    "    scores = validate(scoring_dl, model, top_k=[10], show_progress=True)\n",
    "    cur_metrics = {'loss': np.mean(losses), **scores}\n",
    "    log = f\"Epoch: {epoch} | \" + \" | \".join(map(lambda x: f'{x[0]}: {x[1]:.3f}', cur_metrics.items()))\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "624dfbaffc80753891dd8435b69ccc7ce09dbdd0aaf61853a958149840a42935"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
